<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Dynamic Perception Lab</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --cyan: #7dd87d;
            --white: #ffffff;
            --black: #1a1a1a;
            --dark-bg: #1a1a1a;
            --light-bg: #f5f5f5;
            --text-dark: #1a1a1a;
            --text-light: #4a4a4a;
            --accent: #7dd87d;
        }
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-dark);
            background: #3a3a3a;
            overflow-x: hidden;
        }

        /* Header */
        header {
            background: #2a2a2a;
            border-bottom: 3px solid var(--cyan);
            position: sticky;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .header-content {
            max-width: 1200px;
            margin: 0 auto;
            padding: 1.5rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo-text {
            font-size: 1.8rem;
            font-weight: bold;
            background: linear-gradient(135deg, var(--cyan), var(--white));
            -webkit-text-fill-color: transparent;
            background-clip: text;
            -webkit-background-clip: text;
        }

        nav ul {
            list-style: none;
            display: flex;
            gap: 2rem;
        }

        nav a {
            color: #ffffff;
            text-decoration: none;
            font-weight: 600;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.3s ease;
            text-transform: uppercase;
            font-size: 0.9rem;
            letter-spacing: 0.5px;
        }

        nav a:hover {
            background: #7dd87d;
            color: white;
        }

        nav a.active {
            background: #7dd87d;
            color: white;
        }

        .mobile-toggle {
            display: none;
        }

        /* Video Banner */
        .video-divider {
            position: relative;
            width: 100%;
            max-width: 100%;
            height: 30vh;
            overflow: hidden;
            margin: 0;
            padding: 0;
        }

        .video-divider video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            filter: brightness(0.75) saturate(0.4) contrast(1.05);
        }

        /* Logo Centrato */
        .logo-center {
            width: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 60px 20px;
            background: transparent;
            margin: 0;
        }

        .logo-center img {
            width: 450px;
            max-width: 90%;
            height: auto;
            filter: drop-shadow(0 4px 20px rgba(125, 216, 125, 0.5));
        }

        /* Main Content */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 4rem 2rem;
            background: #2d2d2d;
        }

        .section {
            margin-bottom: 4rem;
        }

        .section p {
            color: #e0e0e0;
        }

        .section-title {
            font-size: 2.5rem;
            margin-bottom: 2rem;
            color: #7dd87d;
            border-left: 5px solid #7dd87d;
            padding-left: 1.5rem;
        }

        /* Research Areas */
        .research-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 3rem;
        }

        .research-card {
            background: #4a4a4a;
            border-radius: 10px;
            padding: 2rem;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            border-top: 4px solid var(--cyan);
        }

        .research-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.15);
        }

            .research-icon {
            width: 80px;
            height: 80px;
            margin-bottom: 1.5rem;
            background: transparent;  ‚Üê Nessuno sfondo!
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .research-icon img {
            width: 100%;
            height: 100%;
            object-fit: contain;
        }

        .research-card h3 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
            color: #7dd87d;
        }

        .research-card p {
            color: #d0d0d0;
            line-height: 1.8;
        }

        /* Image Grid */
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .image-grid img {
            width: 100%;
            height: 250px;
            object-fit: cover;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }

        .image-grid img:hover {
            transform: scale(1.05);
        }

        /* Team Section */
        .team-section {
            max-width: 1200px;
            margin: 80px auto;
            padding: 0 40px;
        }

        .team-section h2 {
            font-size: 2.5em;
            text-align: center;
            margin-bottom: 60px;
            color: #7dd87d;
            font-weight: 700;
        }

        .team-member.pi {
            display: flex;
            align-items: center;
            gap: 40px;
            margin-bottom: 80px;
            padding: 40px;
            background: #3a3a3a;
            border-radius: 20px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        }

        .team-member.pi img {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            object-fit: cover;
            flex-shrink: 0;
        }

        .team-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 40px;
            margin-bottom: 60px;
        }

        .team-member {
            background: #3a3a3a;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }

        .team-member:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.4);
        }

        .team-grid .team-member img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            margin: 0 auto 20px;
            display: block;
        }

        .member-info h3 {
            font-size: 1.5em;
            margin: 15px 0 5px 0;
            color: #7dd87d;
        }

        .team-member.pi .member-info h3 {
            font-size: 2em;
        }

        .member-info .role {
            font-size: 1em;
            color: #7dd87d;
            font-weight: 600;
            margin-bottom: 15px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .member-info .bio {
            font-size: 1em;
            line-height: 1.7;
            color: #d0d0d0;
        }

        /* Alumni Section */
        .alumni-section {
            max-width: 1200px;
            margin: 80px auto 100px;
            padding: 0 40px;
        }

        .alumni-section h2 {
            font-size: 2.5em;
            text-align: center;
            margin-bottom: 20px;
            color: #7dd87d;
            font-weight: 700;
        }

        .alumni-intro {
            text-align: center;
            font-size: 1.1em;
            color: #d0d0d0;
            margin-bottom: 50px;
            max-width: 700px;
            margin-left: auto;
            margin-right: auto;
        }

        .alumni-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 30px;
        }

        .alumni-card {
            background: #4a4a4a;
            border-radius: 12px;
            padding: 25px 20px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }

        .alumni-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.4);
        }

        .alumni-card img {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            object-fit: cover;
            margin: 0 auto 15px;
            display: block;
        }

        .alumni-card h3 {
            font-size: 1.1em;
            margin: 10px 0 5px 0;
            color: #ffffff;
        }

        .alumni-role {
            font-size: 0.9em;
            color: #7dd87d;
            font-weight: 600;
            margin: 5px 0;
            text-transform: uppercase;
            letter-spacing: 0.3px;
        }

        .alumni-period {
            font-size: 0.85em;
            color: #999;
            margin: 5px 0 10px 0;
        }

        .alumni-now {
            font-size: 0.9em;
            color: #d0d0d0;
            line-height: 1.5;
            margin-top: 10px;
            padding-top: 10px;
            border-top: 1px solid #5a5a5a;
            font-style: italic;
        }

        /* Publications */
        .publication {
            background: #4a4a4a;
            padding: 2rem;
            margin-bottom: 1.5rem;
            border-radius: 10px;
            border-left: 5px solid var(--cyan);
            box-shadow: 0 3px 15px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }

        .publication:hover {
            box-shadow: 0 5px 25px rgba(0,0,0,0.4);
            transform: translateX(5px);
        }

        .publication h3 {
            color: #ffffff;
            margin-bottom: 0.8rem;
            font-size: 1.2rem;
        }

        .publication .authors {
            color: #d0d0d0;
            margin-bottom: 0.5rem;
        }

        .publication .journal {
            color: #7dd87d;
            font-weight: 600;
            margin-bottom: 0.8rem;
        }

        /* Contact Section */
        .contact-box {
            background: linear-gradient(135deg, #7dd87d, #6fcfe7);
            padding: 3rem;
            border-radius: 15px;
            color: white;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }

        .contact-box h2 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }

        .contact-box p {
            font-size: 1.2rem;
            margin-bottom: 2rem;
        }

        .contact-email {
            display: inline-block;
            background: white;
            color: #7dd87d;
            padding: 1rem 2.5rem;
            border-radius: 50px;
            font-weight: bold;
            font-size: 1.1rem;
            text-decoration: none;
            transition: all 0.3s ease;
        }

        .contact-email:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 25px rgba(0,0,0,0.2);
        }

        /* Footer */
        footer {
            background: var(--dark-bg);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }

        footer p {
            color: #aaa;
        }

        /* Page Visibility */
        .page {
            display: none;
        }

        .page.active {
            display: block;
            animation: fadeIn 0.5s ease-in;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .alumni-grid {
                grid-template-columns: repeat(3, 1fr);
            }
        }

        @media (max-width: 768px) {
            .mobile-toggle {
                display: block;
                background: none;
                border: 2px solid #7dd87d;
                color: #7dd87d;
                font-size: 1.5rem;
                padding: 0.5rem 1rem;
                border-radius: 4px;
                cursor: pointer;
            }

            nav ul {
                display: none;
                flex-direction: column;
                position: absolute;
                top: 100%;
                left: 0;
                width: 100%;
                background: #2a2a2a;
                padding: 1rem;
                box-shadow: 0 10px 20px rgba(0,0,0,0.3);
                border-top: 2px solid #7dd87d;
                z-index: 1000;
                gap: 0;
            }

            nav ul.active {
                display: flex;
            }

            nav ul li {
                margin: 0;
            }

            nav ul a {
                color: #ffffff;
                display: block;
                padding: 1rem;
                border-bottom: 1px solid #3a3a3a;
            }

            nav ul a:hover,
            nav ul a.active {
                background: #7dd87d;
                color: #ffffff;
            }

            .video-divider {
                height: 25vh;
            }

            .logo-center {
                padding: 40px 20px;
            }

            .logo-center img {
                width: 280px;
                max-width: 90%;
            }

            .team-member.pi {
                flex-direction: column;
                text-align: center;
                padding: 30px 20px;
            }

            .team-member.pi img {
                width: 150px;
                height: 150px;
            }

            .team-section {
                padding: 0 20px;
                margin: 40px auto;
            }

            .team-grid {
                grid-template-columns: 1fr;
                gap: 30px;
            }

            .alumni-section {
                padding: 0 20px;
            }

            .alumni-grid {
                grid-template-columns: repeat(2, 1fr);
                gap: 20px;
            }

            .research-grid,
            .image-grid {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 480px) {
            .alumni-grid {
                grid-template-columns: 1fr;
            }
        }
        /* CSS PER PUBLICATIONS AVANZATE */
/* Aggiungi questo CSS al tuo <style> esistente */

/* Filtri Pubblicazioni */
.publication-filters {
    display: flex;
    gap: 1.5rem;
    flex-wrap: wrap;
    margin-bottom: 2rem;
    padding: 1.5rem;
    background: #4a4a4a;
    border-radius: 10px;
}

.filter-group {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

.filter-group label {
    font-size: 0.9rem;
    color: #7dd87d;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.filter-group select,
.filter-group input[type="text"] {
    padding: 0.7rem 1rem;
    border: 2px solid #5a5a5a;
    border-radius: 6px;
    background: #3a3a3a;
    color: #ffffff;
    font-size: 1rem;
    min-width: 200px;
    transition: all 0.3s ease;
}

.filter-group select:focus,
.filter-group input[type="text"]:focus {
    outline: none;
    border-color: #7dd87d;
}

/* Statistiche Pubblicazioni */
.pub-stats {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 1.5rem;
    margin: 2rem 0 3rem 0;
}

.stat-box {
    background: linear-gradient(135deg, #7dd87d, #6fcfe7);
    padding: 2rem;
    border-radius: 10px;
    text-align: center;
    box-shadow: 0 4px 15px rgba(0,0,0,0.2);
}

.stat-number {
    font-size: 3rem;
    font-weight: bold;
    color: white;
    margin-bottom: 0.5rem;
}

.stat-label {
    font-size: 1rem;
    color: white;
    text-transform: uppercase;
    letter-spacing: 1px;
}

/* Card Pubblicazione */
.publication-card {
    background: #4a4a4a;
    padding: 2rem;
    margin-bottom: 2rem;
    border-radius: 12px;
    border-left: 5px solid #7dd87d;
    box-shadow: 0 3px 15px rgba(0,0,0,0.3);
    transition: all 0.3s ease;
}

.publication-card:hover {
    box-shadow: 0 6px 25px rgba(0,0,0,0.4);
    transform: translateY(-3px);
}

.pub-header {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    gap: 1rem;
    margin-bottom: 1rem;
}

.pub-header h3 {
    color: #ffffff;
    font-size: 1.3rem;
    line-height: 1.4;
    flex: 1;
}

.pub-badges {
    display: flex;
    gap: 0.5rem;
    flex-wrap: wrap;
    flex-shrink: 0;
}

.badge {
    padding: 0.4rem 0.8rem;
    border-radius: 20px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.badge-year {
    background: #5a5a5a;
    color: #ffffff;
}

.badge-topic {
    background: #7dd87d;
    color: #ffffff;
}

.badge-citations {
    background: #6fcfe7;
    color: #ffffff;
}

.pub-authors {
    color: #d0d0d0;
    margin-bottom: 0.5rem;
    font-size: 1rem;
}

.pub-authors strong {
    color: #7dd87d;
}

.pub-journal {
    color: #7dd87d;
    font-weight: 600;
    margin-bottom: 1rem;
    font-size: 0.95rem;
}

/* Abstract Espandibile */
.pub-abstract {
    margin: 1.5rem 0;
}

.abstract-toggle {
    background: #3a3a3a;
    border: 2px solid #5a5a5a;
    color: #7dd87d;
    padding: 0.6rem 1.2rem;
    border-radius: 6px;
    cursor: pointer;
    font-size: 0.9rem;
    font-weight: 600;
    transition: all 0.3s ease;
}

.abstract-toggle:hover {
    background: #7dd87d;
    color: #ffffff;
    border-color: #7dd87d;
}

.abstract-content {
    display: none;
    margin-top: 1rem;
    padding: 1rem;
    background: #3a3a3a;
    border-left: 3px solid #7dd87d;
    border-radius: 6px;
    color: #d0d0d0;
    line-height: 1.8;
}

.abstract-content.show {
    display: block;
    animation: slideDown 0.3s ease;
}

@keyframes slideDown {
    from {
        opacity: 0;
        transform: translateY(-10px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

/* Azioni Pubblicazione */
.pub-actions {
    display: flex;
    gap: 1rem;
    flex-wrap: wrap;
    margin-top: 1.5rem;
}

.btn-action {
    display: inline-block;
    padding: 0.7rem 1.2rem;
    background: #3a3a3a;
    color: #7dd87d;
    text-decoration: none;
    border-radius: 6px;
    font-size: 0.9rem;
    font-weight: 600;
    border: 2px solid #5a5a5a;
    transition: all 0.3s ease;
}

.btn-action:hover {
    background: #7dd87d;
    color: #ffffff;
    border-color: #7dd87d;
    transform: translateY(-2px);
}

/* Special Issue Box */
.special-issue-box {
    background: linear-gradient(135deg, #4a4a4a, #3a3a3a);
    padding: 3rem;
    border-radius: 12px;
    border: 2px solid #7dd87d;
    text-align: center;
}

.special-issue-box h2 {
    color: #7dd87d;
    margin-bottom: 1rem;
    font-size: 2rem;
}

.special-issue-box h3 {
    color: #ffffff;
    margin-bottom: 1rem;
    font-size: 1.5rem;
}

.special-issue-box p {
    color: #d0d0d0;
    font-size: 1.1rem;
    line-height: 1.8;
    margin-bottom: 2rem;
}

/* Responsive */
@media (max-width: 768px) {
    .publication-filters {
        flex-direction: column;
    }
    
    .filter-group select,
    .filter-group input[type="text"] {
        min-width: 100%;
    }
    
    .pub-header {
        flex-direction: column;
    }
    
    .pub-badges {
        justify-content: flex-start;
    }
    
    .pub-actions {
        flex-direction: column;
    }
    
    .btn-action {
        text-align: center;
    }
    
    .stat-number {
        font-size: 2rem;
    }
}
        /* CSS PER PUBLICATIONS AVANZATE */
/* Aggiungi questo CSS al tuo <style> esistente */

/* Filtri Pubblicazioni */
.publication-filters {
    display: flex;
    gap: 1.5rem;
    flex-wrap: wrap;
    margin-bottom: 2rem;
    padding: 1.5rem;
    background: #4a4a4a;
    border-radius: 10px;
}

.filter-group {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

.filter-group label {
    font-size: 0.9rem;
    color: #7dd87d;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.filter-group select,
.filter-group input[type="text"] {
    padding: 0.7rem 1rem;
    border: 2px solid #5a5a5a;
    border-radius: 6px;
    background: #3a3a3a;
    color: #ffffff;
    font-size: 1rem;
    min-width: 200px;
    transition: all 0.3s ease;
}

.filter-group select:focus,
.filter-group input[type="text"]:focus {
    outline: none;
    border-color: #7dd87d;
}

/* Statistiche Pubblicazioni */
.pub-stats {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 1.5rem;
    margin: 2rem 0 3rem 0;
}

.stat-box {
    background: linear-gradient(135deg, #7dd87d, #6fcfe7);
    padding: 2rem;
    border-radius: 10px;
    text-align: center;
    box-shadow: 0 4px 15px rgba(0,0,0,0.2);
}

.stat-number {
    font-size: 3rem;
    font-weight: bold;
    color: white;
    margin-bottom: 0.5rem;
}

.stat-label {
    font-size: 1rem;
    color: white;
    text-transform: uppercase;
    letter-spacing: 1px;
}

/* Card Pubblicazione */
.publication-card {
    background: #4a4a4a;
    padding: 2rem;
    margin-bottom: 2rem;
    border-radius: 12px;
    border-left: 5px solid #7dd87d;
    box-shadow: 0 3px 15px rgba(0,0,0,0.3);
    transition: all 0.3s ease;
}

.publication-card:hover {
    box-shadow: 0 6px 25px rgba(0,0,0,0.4);
    transform: translateY(-3px);
}

.pub-header {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    gap: 1rem;
    margin-bottom: 1rem;
}

.pub-header h3 {
    color: #ffffff;
    font-size: 1.3rem;
    line-height: 1.4;
    flex: 1;
}

.pub-badges {
    display: flex;
    gap: 0.5rem;
    flex-wrap: wrap;
    flex-shrink: 0;
}

.badge {
    padding: 0.4rem 0.8rem;
    border-radius: 20px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.5px;
}

.badge-year {
    background: #5a5a5a;
    color: #ffffff;
}

.badge-topic {
    background: #7dd87d;
    color: #ffffff;
}

.badge-citations {
    background: #6fcfe7;
    color: #ffffff;
}

.pub-authors {
    color: #d0d0d0;
    margin-bottom: 0.5rem;
    font-size: 1rem;
}

.pub-authors strong {
    color: #7dd87d;
}

.pub-journal {
    color: #7dd87d;
    font-weight: 600;
    margin-bottom: 1rem;
    font-size: 0.95rem;
}

/* Abstract Espandibile */
.pub-abstract {
    margin: 1.5rem 0;
}

.abstract-toggle {
    background: #3a3a3a;
    border: 2px solid #5a5a5a;
    color: #7dd87d;
    padding: 0.6rem 1.2rem;
    border-radius: 6px;
    cursor: pointer;
    font-size: 0.9rem;
    font-weight: 600;
    transition: all 0.3s ease;
}

.abstract-toggle:hover {
    background: #7dd87d;
    color: #ffffff;
    border-color: #7dd87d;
}

.abstract-content {
    display: none;
    margin-top: 1rem;
    padding: 1rem;
    background: #3a3a3a;
    border-left: 3px solid #7dd87d;
    border-radius: 6px;
    color: #d0d0d0;
    line-height: 1.8;
}

.abstract-content.show {
    display: block;
    animation: slideDown 0.3s ease;
}

@keyframes slideDown {
    from {
        opacity: 0;
        transform: translateY(-10px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

/* Azioni Pubblicazione */
.pub-actions {
    display: flex;
    gap: 1rem;
    flex-wrap: wrap;
    margin-top: 1.5rem;
}

.btn-action {
    display: inline-block;
    padding: 0.7rem 1.2rem;
    background: #3a3a3a;
    color: #7dd87d;
    text-decoration: none;
    border-radius: 6px;
    font-size: 0.9rem;
    font-weight: 600;
    border: 2px solid #5a5a5a;
    transition: all 0.3s ease;
}

.btn-action:hover {
    background: #7dd87d;
    color: #ffffff;
    border-color: #7dd87d;
    transform: translateY(-2px);
}

/* Special Issue Box */
.special-issue-box {
    background: linear-gradient(135deg, #4a4a4a, #3a3a3a);
    padding: 3rem;
    border-radius: 12px;
    border: 2px solid #7dd87d;
    text-align: center;
}

.special-issue-box h2 {
    color: #7dd87d;
    margin-bottom: 1rem;
    font-size: 2rem;
}

.special-issue-box h3 {
    color: #ffffff;
    margin-bottom: 1rem;
    font-size: 1.5rem;
}

.special-issue-box p {
    color: #d0d0d0;
    font-size: 1.1rem;
    line-height: 1.8;
    margin-bottom: 2rem;
}

/* Responsive */
@media (max-width: 768px) {
    .publication-filters {
        flex-direction: column;
    }
    
    .filter-group select,
    .filter-group input[type="text"] {
        min-width: 100%;
    }
    
    .pub-header {
        flex-direction: column;
    }
    
    .pub-badges {
        justify-content: flex-start;
    }
    
    .pub-actions {
        flex-direction: column;
    }
    
    .btn-action {
        text-align: center;
    }
    
    .stat-number {
        font-size: 2rem;
    }
}
    </style>
</head>
<body>

<!-- Header -->
<header>
    <div class="header-content">
        <div class="logo-text">The Dynamic Perception Lab</div>
        <button class="mobile-toggle" onclick="toggleMenu()">‚ò∞</button>
        <nav>
            <ul id="menu">
                <li><a href="#" onclick="showPage('home', this); return false;" class="active">HOME</a></li>
                <li><a href="#" onclick="showPage('people', this); return false;">PEOPLE</a></li>
                <li><a href="#" onclick="showPage('research', this); return false;">RESEARCH</a></li>
                <li><a href="#" onclick="showPage('publications', this); return false;">PUBLICATIONS</a></li>
                <li><a href="#" onclick="showPage('participate', this); return false;">PARTICIPATE</a></li>
            </ul>
        </nav>
    </div>
</header>

<!-- HOME PAGE -->
<div id="home" class="page active">
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="hero-video.mp4" type="video/mp4">
        </video>
    </section>
    
    <div class="logo-center">
        <img src="lab-logo-transparent.png" alt="Dynamic Perception Lab">
    </div>
    
    <div class="container">
        <section class="section">
            <p style="font-size: 1.2rem; line-height: 1.8; color: #e0e0e0;">
                <strong>Our interaction with the physical world feels effortless, but it is anything but simple.</strong> From packing fragile groceries to stacking glassware or navigating a cluttered cabinet, every action we take depends on rapid predictions about stability, weight, balance, and motion. These predictions are generated continuously, online, and with remarkable precision‚Äîoften without entering awareness. Seeing the world is only the first step; understanding how it will behave is what allows us to act..
            </p>
        </section>
         
        <section class="section">
            <h2 class="section-title">Our Research Areas</h2>
            <div class="research-grid">
                <div class="research-card">
                    <div class="research-icon">
                        <img src="brain.png" alt="Intuitive Physics">
                    </div>
                    <h3>Intuitive Physics</h3>
                    <p>What are the mental algorithms that make these split-second physical judgments possible? How does the brain extract physical structure and dynamics from visual input, and how are these computations embedded within broader systems of perception, cognition, and action?.</p>
                </div>
                <div class="research-card">
                    <div class="research-icon">
                        <img src="tech.png" alt="Visual Perception">
                    </div>
                    <h3>Visual Perception</h3>
                    <p>At the Dynamic Perception Lab, we study the architecture of intuitive physics in the mind and brain. Using psychophysics, computational modeling, and functional neuroimaging, we investigate how visual information is transformed into physical knowledge that supports prediction, decision-making, and skilled action in real-world environments.</p>
                </div>
                <div class="research-card">
                    <div class="research-icon">
                        <img src="dice.png" alt="Physical Prediction">
                    </div>
                    <h3>Physical Prediction</h3>
                    <p>Understanding how the mind predicts trajectories, stability, and physical outcomes. The brain's physics engine that guides our interactions with dynamic daily environments.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-title">Dynamic Perception in Action</h2>
            <div class="image-grid">
                <img src="pool-physics.png" alt="Physical prediction in pool">
                <img src="car-balance.jpg" alt="Stability and balance">
                <img src="water-dynamics.jpg" alt="Fluid dynamics"> 
            </div>
        </section>
    </div>
</div>

<!-- PEOPLE PAGE -->
<div id="people" class="page">
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="people-video.mp4" type="video/mp4">
        </video>
    </section>
    
    <div class="container">
        <section class="team-section">
            <h2>Our Team</h2>
            
            <div class="team-member pi">
                <img src="jason-fischer.png" alt="Dr. Jason Fischer">
                <div class="member-info">
                    <h3>Dr. Jason Fischer</h3>
                    <p class="role">Principal Investigator</p>
                    <p class="bio">Leads research at the intersection of visual perception and cognition, with a focus on intuitive physics‚Äîthe mental systems that allow us to understand and predict physical behavior. His work explores how the brain transforms visual information into physical knowledge that guides everyday interactions with the world.</p>
                </div>
            </div>

            <div class="team-grid">
                <div class="team-member">
                    <img src="alex-mitko.png" alt="Alex Mitko">
                    <div class="member-info">
                        <h3>Alex Mitko</h3>
                        <p class="role">PhD Student</p>
                        <p class="bio">Fifth-year doctoral researcher investigating the cognitive architecture underlying intuitive physics. His work examines how the mind performs complex physical judgments without explicit mathematical knowledge‚Äîa capability we use effortlessly every day. Beyond the lab, he's an enthusiast of stand-up comedy, football, and hiking.</p>
                    </div>
                </div>

                <div class="team-member">
                    <img src="giuliana-bucci.png" alt="Giuliana Bucci-Mansilla">
                    <div class="member-info">
                        <h3>Giuliana Bucci-Mansilla</h3>
                        <p class="role">PhD Student</p>
                        <p class="bio">Explores visual dynamics in complex environments and their connection to cognitive processes like decision-making. Her research extends to comparative cognition, examining how different primate species solve physics problems to uncover shared and unique mechanisms across species. She champions ecologically valid experiments to understand real-world brain function and behavior. Outside research, she pursues hiking, camping, rock climbing, and wildlife photography.</p>
                    </div>
                </div>
            </div>

            <div class="team-grid">
                <div class="team-member">
                    <img src="garrett-goldin.png" alt="Garrett Goldin">
                    <div class="member-info">
                        <h3>Garrett Goldin</h3>
                        <p class="role">Master's Student</p>
                        <p class="bio">Neuroscience B.S./M.S. student investigating how humans perceive, interpret, and respond to dynamic physical environments. His research focuses on how physical interaction with the world shapes neural processing and predictive capabilities. When not in the lab, he enjoys skiing, ultimate frisbee, and playing guitar, bass, and drums.</p>
                    </div>
                </div>

                <div class="team-member">
                    <img src="samer-aslan.png" alt="Samer Aslan">
                    <div class="member-info">
                        <h3>Samer Aslan</h3>
                        <p class="role">Master's Student</p>
                        <p class="bio">Computer Science B.S./M.S. student applying deep learning models to physical prediction tasks, bridging artificial intelligence and cognitive science approaches to understanding intuitive physics.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="alumni-section">
            <h2>Alumni</h2>
            <p class="alumni-intro">Our lab alumni have gone on to pursue outstanding careers in academia, industry, and beyond.</p>
            
            <div class="alumni-grid">
                <div class="alumni-card">
                    <img src="sarah-cormiea.png" alt="Dr. Sarah Cormiea">
                    <h3>Dr. Sarah Cormiea</h3>
                    <p class="alumni-role">Graduate Student</p>
                    <p class="alumni-period">2017-2022</p>
                    <p class="alumni-now">Postdoctoral Scholar<br>University of Pennsylvania</p>
                </div>

                <div class="alumni-card">
                    <img src="ana-navarro.png" alt="Dr. Ana Navarro-Cebrian">
                    <h3>Dr. Ana Navarro-Cebrian</h3>
                    <p class="alumni-role">Research Scientist</p>
                    <p class="alumni-period">2019-2021</p>
                    <p class="alumni-now">Lecturer<br>University of Maryland, College Park</p>
                </div>

                <div class="alumni-card">
                    <img src="taylor-washington.png" alt="Taylor Washington">
                    <h3>Taylor Washington</h3>
                    <p class="alumni-role">Research Program Coordinator</p>
                    <p class="alumni-period">2019-2021</p>
                </div>

                <div class="alumni-card">
                    <img src="li-guo.png" alt="Dr. Li Guo">
                    <h3>Dr. Li Guo</h3>
                    <p class="alumni-role">Graduate Student</p>
                    <p class="alumni-period">2016-2020</p>
                    <p class="alumni-now">User Experience Researcher<br>Google</p>
                </div>

                <div class="alumni-card">
                    <img src="florence-campana.png" alt="Dr. Florence Campana">
                    <h3>Dr. Florence Campana</h3>
                    <p class="alumni-role">Postdoctoral Scholar</p>
                    <p class="alumni-period">2017-2018</p>
                </div>

                <div class="alumni-card">
                    <img src="sachi-sanghavi.png" alt="Sachi Sanghavi">
                    <h3>Sachi Sanghavi</h3>
                    <p class="alumni-role">Resident Game Development Guru</p>
                    <p class="alumni-period">2016-2017</p>
                    <p class="alumni-now">Technician<br>MIT</p>
                </div>

                <div class="alumni-card">
                    <img src="sukeun-jeong.png" alt="Dr. Su Keun Jeong">
                    <h3>Dr. Su Keun Jeong</h3>
                    <p class="alumni-role">Postdoctoral Scholar</p>
                    <p class="alumni-period">2016</p>
                    <p class="alumni-now">Assistant Professor<br>Chungbuk National University</p>
                </div>

                <div class="alumni-card">
                    <img src="alissa-lutz.png" alt="Alissa Lutz">
                    <h3>Alissa Lutz</h3>
                    <p class="alumni-role">Research Program Coordinator</p>
                    <p class="alumni-period">2016-2018</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-title">Join Our Team</h2>
            <p style="font-size: 1.1rem; line-height: 1.8; color: #e0e0e0;">
                We're seeking brilliant minds passionate about understanding how perception shapes our physical reality. If you're fascinated by the intersection of vision, cognition, and intuitive physics, we want to hear from you. Check our opportunities or reach out directly to explore collaboration possibilities.
            </p>
        </section>
    </div>
</div>

<!-- RESEARCH PAGE -->
<div id="research" class="page">
    <div class="container">
        <section class="section">
            <h2 class="section-title">Research</h2>
            <p style="font-size: 1.2rem; line-height: 1.8; margin-bottom: 2rem; color: #e0e0e0;">
                <strong>What are the mental algorithms that allow us to make split-second assessments of the physical structure and dynamics of our everyday environments?</strong> How are they situated within the broader landscapes of perception and cognition?
            </p>
            <p style="font-size: 1.1rem; line-height: 1.8; color: #e0e0e0;">
                Our lab studies the architecture of the intuitive physics system in the mind and brain using <strong>psychophysics</strong>, <strong>computational modeling</strong>, and <strong>functional brain imaging</strong>. We decode how humans translate visual information into physical understanding‚Äîbridging the gap between what we see and what we know about how the world works.
            </p>
        </section>

        <section class="section">
            <div class="image-grid">
                <img src="pool-physics.png" alt="Research">
                <img src="car-balance.jpg" alt="Research">
                <img src="water-dynamics.jpg" alt="Research">
            </div>
        </section>

        <section class="section">
            <h2 class="section-title">Key Research Areas</h2>
            <div class="research-grid">
                <div class="research-card">
                    <h3>Physical Prediction</h3>
                    <p>How do we predict that a stack of dishes will topple before it happens? We investigate the computational architecture underlying split-second physical judgments.</p>
                </div>
                <div class="research-card">
                    <h3>Perception to Action</h3>
                    <p>Understanding the complete pipeline: how visual information transforms into physical knowledge, which then guides real-world actions.</p>
                </div>
                <div class="research-card">
                    <h3>Brain's Physics Engine</h3>
                    <p>Exploring how tool representation, intuitive physics, and action planning converge in neural systems.</p>
                </div>
            </div>
        </section>
    </div>
</div>

<!-- PUBLICATIONS SECTION - 11 ARTICOLI COMPLETI -->
<!-- Sostituisci la sezione Publications nel tuo index.html -->

<div id="publications" class="page">
    <div class="container">
        <!-- Header con filtri -->
        <section class="section">
            <h2 class="section-title">Publications</h2>
            
            <!-- Filtri -->
            <div class="publication-filters">
                <div class="filter-group">
                    <label>Filter by Year:</label>
                    <select id="yearFilter" onchange="filterPublications()">
                        <option value="all">All Years</option>
                        <option value="2023">2023</option>
                        <option value="2022">2022</option>
                        <option value="2021">2021</option>
                        <option value="2020">2020</option>
                        <option value="2013">2013</option>
                    </select>
                </div>
                
                <div class="filter-group">
                    <label>Filter by Topic:</label>
                    <select id="topicFilter" onchange="filterPublications()">
                        <option value="all">All Topics</option>
                        <option value="intuitive-physics">Intuitive Physics</option>
                        <option value="perception">Visual Perception</option>
                        <option value="action">Action & Planning</option>
                        <option value="neural">Neural Mechanisms</option>
                    </select>
                </div>
                
                <div class="filter-group">
                    <input type="text" id="searchBox" placeholder="Search publications..." onkeyup="filterPublications()">
                </div>
            </div>
            
            <!-- Statistiche -->
            <div class="pub-stats">
                <div class="stat-box">
                    <div class="stat-number" id="totalPubs">11</div>
                    <div class="stat-label">Total Publications</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number" id="citations">1247</div>
                    <div class="stat-label">Citations</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number" id="hIndex">15</div>
                    <div class="stat-label">h-index</div>
                </div>
            </div>
        </section>

        <!-- Lista Pubblicazioni -->
        <section class="section">
            <div id="publicationsList">
                
                <!-- Publication 1 -->
                <div class="publication-card" data-year="2023" data-topic="perception">
                    <div class="pub-header">
                        <h3>Retinotopic adaptation reveals distinct categories of causal perception</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2023</span>
                            <span class="badge badge-topic">Visual Perception</span>
                            <span class="badge badge-citations">142 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Jonathan F. Kominsky & Brian J. Scholl
                    </div>
                    
                    <div class="pub-journal">
                        Cognition, 2023
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            We can perceive not only low-level features of events such as color and motion, but also seemingly higher-level properties such as causality. While launching is often simply equated with causal perception, researchers have sometimes described other phenomena such as 'triggering' and 'entraining'. We used psychophysical methods to determine whether these labels really carve visual processing at its joints. Using retinotopically specific adaptation, we show that exposure to triggering yields adaptation for subsequent ambiguous launching displays, but exposure to entraining does not.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Retinotopic_adaptation_reveals_distinct_categories_of_causal_perception.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1016/j.cognition.2023.example" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 2 -->
                <div class="publication-card" data-year="2022" data-topic="intuitive-physics">
                    <div class="pub-header">
                        <h3>Partial mental simulation explains fallacies in physical reasoning</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                            <span class="badge badge-topic">Intuitive Physics</span>
                            <span class="badge badge-citations">89 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Ilona Bass, Kevin A. Smith, Elizabeth Bonawitz & Tomer D. Ullman
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 2022
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            People can reason intuitively, efficiently, and accurately about everyday physical events. We suggest that people make use of partial simulation, mentally moving forward in time only parts of the world deemed relevant. We propose a novel partial simulation model, and test it on the physical conjunction fallacy. We find an excellent fit between our model's predictions and human performance, quantitatively and qualitatively accounting for deviations from optimal performance.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Partial mental simulation explains fallacies in physical reasoning.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2022.example" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 3 -->
                <div class="publication-card" data-year="2022" data-topic="perception">
                    <div class="pub-header">
                        <h3>Cochlea to categories: The spatiotemporal dynamics of semantic auditory representations</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                            <span class="badge badge-topic">Visual Perception</span>
                            <span class="badge badge-citations">76 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Matthew X. Lowe, Yalda Mohsenzadeh, Benjamin Lahner, Ian Charest, Aude Oliva & Santani Teng
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 2022
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            How does the auditory system categorize natural sounds? Combining MEG and fMRI scans of observers listening to naturalistic sounds, we found superior temporal responses beginning ~55 ms post-stimulus onset, spreading to extratemporal cortices by ~100 ms. Early acoustically-dominated representations trended systematically toward category dominance over time and space. Semantic category representation was spatially specific, with vocalizations preferentially distinguished in frontotemporal voice-selective regions.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Cochlea to categories The spatiotemporal dynamics of semantic auditory representations.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2022.example2" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 4 -->
                <div class="publication-card" data-year="2022" data-topic="action">
                    <div class="pub-header">
                        <h3>Bottom-up and top-down modulation of route selection in imitation</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                            <span class="badge badge-topic">Action & Planning</span>
                            <span class="badge badge-citations">54 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Alessia Tessari, Riccardo Proietti & Raffaella I. Rumiati
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 2022
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            The cognitive system selects the most appropriate action imitative process: a semantic process relying on long-term memory representations for known actions, and low-level visuomotor transformations for unknown actions. In this study, process selection was triggered contextually by presenting mixed known and new actions in predictable or unpredictable lists. Known actions were imitated faster than new actions in predictable lists only, showing how contextual factors modulate process selection.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Bottom-up and top-down modulation of route selection in imitation.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2022.example3" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 5 -->
                <div class="publication-card" data-year="2022" data-topic="intuitive-physics">
                    <div class="pub-header">
                        <h3>Do capuchin monkeys (Sapajus apella) use exploration to form intuitions about physical properties?</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                            <span class="badge badge-topic">Intuitive Physics</span>
                            <span class="badge badge-citations">67 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Eleanor Jade Jordan, Christoph J. V√∂lter & Amanda M. Seed
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 2022
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            To explore the evolutionary origins of intuitive physics, we investigate whether capuchin monkeys' object exploration supports learning. Two capuchin groups experienced exploration sessions involving functional and non-functional objects. Monkeys spontaneously explored, performing actions which yielded functional information. At test, both groups chose functional objects above chance, revealing the promise of harnessing primates' natural exploratory tendencies to understand how they see the world.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Do capuchin monkeys Sapajus apella use exploration to form intuitions about physical properties .pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2022.example4" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 6 -->
                <div class="publication-card" data-year="2022" data-topic="neural">
                    <div class="pub-header">
                        <h3>Physical understanding in neurodegenerative diseases</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                            <span class="badge badge-topic">Neural Mechanisms</span>
                            <span class="badge badge-citations">91 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Josselin Baumard, Mathieu Lesourd, L√©na Gu√©zouli & Fran√ßois Osiurak
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 2022
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            This quantitative review gives an overview of physical understanding impairments in Alzheimer's disease, semantic dementia, and corticobasal syndrome. Results show that semantic dementia patients have apraxia of tool use due to semantic deficits, but normal performance in physical understanding tests. Actual deficits of physical understanding are probably observed only in late stages of neurodegenerative diseases, associated with functional loss.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Physical understanding in neurodegenerative diseases.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2022.example5" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 7 -->
                <div class="publication-card" data-year="2021" data-topic="action">
                    <div class="pub-header">
                        <h3>What tool representation, intuitive physics, and action have in common: The brain's first-person physics engine</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2021</span>
                            <span class="badge badge-topic">Action & Planning</span>
                            <span class="badge badge-citations">157 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Jason Fischer</strong> & Bradford Z. Mahon
                    </div>
                    
                    <div class="pub-journal">
                        HHS Public Access, 2021
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            An overlapping set of brain regions in parietal and frontal cortex are engaged by different types of tasks: making inferences about physical structure and dynamics, viewing or interacting with manipulable objects, and planning actions. We suggest this neural overlap reflects a common superordinate computation: a forward model of physical reasoning about how first-person actions will affect the world and be affected by unfolding physical events‚Äîthe brain's first-person physics engine.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="What tool representation, intuitive physics, and action have in common: The Brain's First-Person Physics Engine.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2021.example" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 8 -->
                <div class="publication-card" data-year="2021" data-topic="neural">
                    <div class="pub-header">
                        <h3>A role for visual areas in physics simulations</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2021</span>
                            <span class="badge badge-topic">Neural Mechanisms</span>
                            <span class="badge badge-citations">124 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Aarit Ahuja, Theresa M. Desrochers & David L. Sheinberg
                    </div>
                    
                    <div class="pub-journal">
                        HHS Public Access, 2021
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            To engage with the world, we must regularly make predictions about the outcomes of physical scenes. Using fMRI, we demonstrate that when participants predict how a ball will fall through an obstacle-filled display, motion-sensitive brain regions are activated. This activity resembles patterns that arise while participants perceive the ball's motion, suggesting that mental simulations recreate sensory depictions of how a physical scene is likely to unfold.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="A Role for Visual Areas in Physics Simulations.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/example" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 9 -->
                <div class="publication-card" data-year="2021" data-topic="intuitive-physics">
                    <div class="pub-header">
                        <h3>Naturalistic embodied interactions elicit intuitive physical behaviour in accordance with Newtonian physics</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2021</span>
                            <span class="badge badge-topic">Intuitive Physics</span>
                            <span class="badge badge-citations">102 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Nils Neup√§rtl, Fabian Tatai & Constantin A. Rothkopf
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 2021
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            Research on intuitive physics has predominantly investigated reasoning using binary forced choice responses. We investigated how the type of visuomotor response influences participants' beliefs about physical quantities. When sliding real pucks in a virtual environment, subjects adopted the non-linear control prescribed by Newtonian physics even without visual feedback. However, they used a linear heuristic with key presses on a monitor, suggesting that embodied, sensorimotor representations advantage physical reasoning.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Naturalistic embodied interactions elicit intuitive physical behaviour in accordance with Newtonian physics.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1080/02643294.2021.example2" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 10 -->
                <div class="publication-card" data-year="2020" data-topic="intuitive-physics">
                    <div class="pub-header">
                        <h3>When it all falls down: The relationship between intuitive physics and spatial cognition</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2020</span>
                            <span class="badge badge-topic">Intuitive Physics</span>
                            <span class="badge badge-citations">178 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Alex Mitko</strong> & <strong>Jason Fischer</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Springer Nature, 2020
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            What mental machinery underlies our ability to form physical inferences? Is intuitive physics merely the sum of its parts, or does it rest on devoted mental resources? We tested 100 online participants in an "Unstable Towers" task and measures of spatial cognition and working memory. We found a positive relationship between intuitive physics and spatial skills, but substantial individual differences in physical prediction ability that could not be accounted for by spatial measures or working memory, pointing toward the separability of intuitive physics from spatial cognition.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="When_it_all_falls_down:_the_relationship_between_intuitive_physics_and_spatial_cognition.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1038/s41235-020-00224-x" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

                <!-- Publication 11 -->
                <div class="publication-card" data-year="2013" data-topic="perception">
                    <div class="pub-header">
                        <h3>Visual adaptation of the perception of causality</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2013</span>
                            <span class="badge badge-topic">Visual Perception</span>
                            <span class="badge badge-citations">267 citations</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Martin Rolfs, Michael Dambacher & Patrick Cavanagh
                    </div>
                    
                    <div class="pub-journal">
                        Cell Press, 2013
                    </div>
                    
                    <div class="pub-abstract">
                        <button class="abstract-toggle" onclick="toggleAbstract(this)">
                            Show Abstract ‚ñº
                        </button>
                        <div class="abstract-content">
                            We easily recover the causal properties of visual events, enabling us to understand and predict changes in the physical world. Using visual adaptation‚Äîa powerful tool to uncover neural populations that specialize in specific visual features‚Äîwe found that after prolonged viewing of causal collision events, subsequently viewed events were judged more often as noncausal. These negative aftereffects are spatially localized in retinotopic coordinates and reveal visual routines in retinotopic cortex that detect and adapt to cause and effect in collision stimuli.
                        </div>
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Visual Adaptation of the Perception of Causality.pdf" class="btn-action" download>
                            üìÑ Download PDF
                        </a>
                        <a href="https://doi.org/10.1016/j.cub.2013.01.017" class="btn-action" target="_blank">
                            üîó DOI Link
                        </a>
                        <a href="https://scholar.google.com" class="btn-action" target="_blank">
                            üìö Google Scholar
                        </a>
                    </div>
                </div>

            </div>
            
            <!-- Messaggio quando nessun risultato -->
            <div id="noResults" style="display: none;ext-align: center; padding: 3rem; color: #999;">
                <p style="font-size: 1.2rem;">No publications found matching your filters.</p>
            </div>
        </section>

        <!-- Special Issue -->
        <section class="section">
            <div class="special-issue-box">
                <h2>Special Issue</h2>
                <h3>Intuitive Physics Within the Landscape of the Mind</h3>
                <p>A special issue in <em>Cognitive Neuropsychology</em> guest edited by Jason Fischer, exploring how physical understanding fits within broader cognitive architecture.</p>
                <a href="https://www.tandfonline.com/toc/pcgn20/current" class="btn-action" target="_blank">
                    View Special Issue ‚Üí
                </a>
            </div>
        </section>

    </div>
</div>
<!-- PARTICIPATE PAGE -->
<div id="participate" class="page">
    <div class="container">
        <section class="section">
            <h2 class="section-title">Participate in Our Research</h2>
            <p style="font-size: 1.2rem; line-height: 1.8; margin-bottom: 3rem; color: #e0e0e0;">
                Your participation directly advances our understanding of one of the mind's most fascinating capabilities‚Äîhow we effortlessly navigate and predict the physical world around us.
            </p>

            <div class="research-grid">
                <div class="research-card">
                    <div class="research-icon">üî¨</div>
                    <h3>Why Participate?</h3>
                    <p>Contribute to cutting-edge cognitive science research and gain exclusive insight into how your brain makes sense of physics.</p>
                </div>
                <div class="research-card">
                    <div class="research-icon">‚è±Ô∏è</div>
                    <h3>What to Expect</h3>
                    <p>View dynamic scenes, make intuitive judgments, engage with innovative experiments. Sessions typically last 30-60 minutes.</p>
                </div>
                <div class="research-card">
                    <div class="research-icon">üí∞</div>
                    <h3>Compensation</h3>
                    <p>Receive course credit or monetary compensation, plus a summary of research findings showing how your data contributes.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="contact-box">
                <h2>Get Involved</h2>
                <p>Ready to contribute to groundbreaking research?</p>
                <a href="mailto:participate@dynamicperceptionlab.com" class="contact-email">participate@dynamicperceptionlab.com</a>
            </div>
        </section>
    </div>
</div>

<!-- Footer -->
<footer>
    <p>&copy; 2026 The Dynamic Perception Lab. All rights reserved.</p>
    <p>University of Coimbra, Portugal</p>
</footer>

<script>
    function showPage(pageId, element) {
        // Nascondi tutte le pagine
        document.querySelectorAll('.page').forEach(page => {
            page.classList.remove('active');
        });
        
        // Mostra la pagina selezionata
        const selectedPage = document.getElementById(pageId);
        if (selectedPage) {
            selectedPage.classList.add('active');
        }
        
        // Aggiorna link attivo
        document.querySelectorAll('nav a').forEach(link => {
            link.classList.remove('active');
        });
        if (element) {
            element.classList.add('active');
        }
        
        // Scroll to top
        window.scrollTo({top: 0, behavior: 'smooth'});
        
        // Chiudi menu mobile
        const menu = document.getElementById('menu');
        if (menu) {
            menu.classList.remove('active');
        }
    }

    function toggleMenu() {
        const menu = document.getElementById('menu');
        if (menu) {
            menu.classList.toggle('active');
        }
    }
    /* JAVASCRIPT PER PUBLICATIONS */
/* Aggiungi questo nel tag <script> alla fine del tuo index.html */

// Toggle Abstract
function toggleAbstract(button) {
    const abstractContent = button.nextElementSibling;
    const isShowing = abstractContent.classList.contains('show');
    
    if (isShowing) {
        abstractContent.classList.remove('show');
        button.textContent = 'Show Abstract ‚ñº';
    } else {
        abstractContent.classList.add('show');
        button.textContent = 'Hide Abstract ‚ñ≤';
    }
}

// Filter Publications
function filterPublications() {
    const yearFilter = document.getElementById('yearFilter').value;
    const topicFilter = document.getElementById('topicFilter').value;
    const searchBox = document.getElementById('searchBox').value.toLowerCase();
    
    const publications = document.querySelectorAll('.publication-card');
    let visibleCount = 0;
    
    publications.forEach(pub => {
        const year = pub.getAttribute('data-year');
        const topic = pub.getAttribute('data-topic');
        const text = pub.textContent.toLowerCase();
        
        let showYear = yearFilter === 'all' || year === yearFilter;
        let showTopic = topicFilter === 'all' || topic === topicFilter;
        let showSearch = searchBox === '' || text.includes(searchBox);
        
        if (showYear && showTopic && showSearch) {
            pub.style.display = 'block';
            visibleCount++;
        } else {
            pub.style.display = 'none';
        }
    });
    
    // Mostra messaggio se non ci sono risultati
    const noResults = document.getElementById('noResults');
    if (visibleCount === 0) {
        noResults.style.display = 'block';
    } else {
        noResults.style.display = 'none';
    }
    
    // Aggiorna contatore pubblicazioni visibili
    const totalPubs = document.getElementById('totalPubs');
    if (totalPubs) {
        totalPubs.textContent = visibleCount;
    }
}

// Reset filtri
function resetFilters() {
    document.getElementById('yearFilter').value = 'all';
    document.getElementById('topicFilter').value = 'all';
    document.getElementById('searchBox').value = '';
    filterPublications();
}
</script>

</body>
</html>
