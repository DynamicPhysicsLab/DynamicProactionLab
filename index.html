<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Dynamic Perception Lab</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --cyan: #7dd87d;
            --white: #ffffff;
            --black: #1a1a1a;
            --dark-bg: #1a1a1a;
            --light-bg: #f5f5f5;
            --text-dark: #1a1a1a;
            --text-light: #4a4a4a;
            --accent: #7dd87d;
        }
        
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            color: var(--text-dark);
            background: #3a3a3a;
            overflow-x: hidden;
        }

        /* Header */
        header {
            background: #2a2a2a;
            border-bottom: 3px solid var(--cyan);
            position: sticky;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .header-content {
            max-width: 1200px;
            margin: 0 auto;
            padding: 1.5rem 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo-text {
            font-size: 1.8rem;
            font-weight: bold;
            background: linear-gradient(135deg, var(--cyan), var(--white));
            -webkit-text-fill-color: transparent;
            background-clip: text;
            -webkit-background-clip: text;
        }

        nav ul {
            list-style: none;
            display: flex;
            gap: 2rem;
        }

        nav a {
            color: #ffffff;
            text-decoration: none;
            font-weight: 600;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: all 0.3s ease;
            text-transform: uppercase;
            font-size: 0.9rem;
            letter-spacing: 0.5px;
        }

        nav a:hover {
            background: #7dd87d;
            color: white;
        }

        nav a.active {
            background: #7dd87d;
            color: white;
        }

        .mobile-toggle {
            display: none;
        }

        /* Video Banner */
        .video-divider {
            position: relative;
            width: 100%;
            max-width: 100%;
            height: 30vh;
            overflow: hidden;
            margin: 0;
            padding: 0;
        }

        .video-divider video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            filter: brightness(0.75) saturate(0.4) contrast(1.05);
        }

        /* Altezza specifica per Research, Publications e Participate */
        #research .video-divider,
        #publications .video-divider,
        #participate .video-divider {
            height: 20vh;
        }

        /* Logo Centrato */
        .logo-center {
            width: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 60px 20px;
            background: transparent;
            margin: 0;
        }

        .logo-center img {
            width: 450px;
            max-width: 90%;
            height: auto;
            filter: drop-shadow(0 4px 20px rgba(125, 216, 125, 0.5));
        }

        /* Main Content */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 4rem 2rem;
            background: #2d2d2d;
        }

        .section {
            margin-bottom: 4rem;
        }

        .section p {
            color: #e0e0e0;
        }

        .section-title {
            font-size: 2.5rem;
            margin-bottom: 2rem;
            color: #7dd87d;
            border-left: 5px solid #7dd87d;
            padding-left: 1.5rem;
        }

        /* Research Areas */
        .research-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 3rem;
        }

        .research-card {
            background: #4a4a4a;
            border-radius: 10px;
            padding: 2rem;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            border-top: 4px solid var(--cyan);
        }

        .research-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.15);
        }

        .research-icon {
            width: 80px;
            height: 80px;
            margin-bottom: 1.5rem;
            background: transparent;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 3rem;
        }

        .research-icon img {
            width: 100%;
            height: 100%;
            object-fit: contain;
        }

        .research-card h3 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
            color: #7dd87d;
        }

        .research-card p {
            color: #d0d0d0;
            line-height: 1.8;
        }

        /* Image Grid */
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .image-grid img {
            width: 100%;
            height: 250px;
            object-fit: cover;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }

        .image-grid img:hover {
            transform: scale(1.05);
        }

        /* Research Text Content - Font sottile */
        .research-text-content {
            font-size: 1.05rem;
            line-height: 1.9;
            color: #d0d0d0;
            font-weight: 300;
        }

        .research-text-content p {
            margin-bottom: 1.5rem;
            text-align: justify;
            font-weight: 300;
        }

        .research-text-content h3 {
            color: #7dd87d;
            font-size: 1.5rem;
            margin: 2.5rem 0 1.5rem 0;
            font-weight: 500;
            border-bottom: 2px solid #7dd87d;
            padding-bottom: 0.5rem;
            clear: both;
        }

        /* Immagini inline integrate nel testo */
        .research-inline-image {
            float: right;
            width: 40%;
            max-width: 400px;
            margin: 0 0 1.5rem 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        }

        .research-inline-image img {
            width: 100%;
            height: auto;
            display: block;
            border-radius: 10px;
            background: transparent;
            object-fit: contain;
        }

        .research-inline-image:nth-of-type(2n) {
            float: left;
            margin: 0 2rem 1.5rem 0;
        }

        /* Team Section */
        .team-section {
            max-width: 1200px;
            margin: 80px auto;
            padding: 0 40px;
        }

        .team-section h2 {
            font-size: 2.5em;
            text-align: center;
            margin-bottom: 60px;
            color: #7dd87d;
            font-weight: 700;
        }

        .team-member.pi {
            display: flex;
            align-items: center;
            gap: 40px;
            margin-bottom: 80px;
            padding: 40px;
            background: #3a3a3a;
            border-radius: 20px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        }

        .team-member.pi img {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            object-fit: cover;
            flex-shrink: 0;
        }

        .team-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 40px;
            margin-bottom: 60px;
        }

        .team-member {
            background: #3a3a3a;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 2px 15px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }

        .team-member:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.4);
        }

        .team-grid .team-member img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            margin: 0 auto 20px;
            display: block;
        }

        .member-info h3 {
            font-size: 1.5em;
            margin: 15px 0 5px 0;
            color: #7dd87d;
        }

        .team-member.pi .member-info h3 {
            font-size: 2em;
        }

        .member-info .role {
            font-size: 1em;
            color: #7dd87d;
            font-weight: 600;
            margin-bottom: 15px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .member-info .bio {
            font-size: 1em;
            line-height: 1.7;
            color: #d0d0d0;
        }

        /* Alumni Section */
        .alumni-section {
            max-width: 1200px;
            margin: 80px auto 100px;
            padding: 0 40px;
        }

        .alumni-section h2 {
            font-size: 2.5em;
            text-align: center;
            margin-bottom: 20px;
            color: #7dd87d;
            font-weight: 700;
        }

        .alumni-intro {
            text-align: center;
            font-size: 1.1em;
            color: #d0d0d0;
            margin-bottom: 50px;
            max-width: 700px;
            margin-left: auto;
            margin-right: auto;
        }

        .alumni-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 30px;
        }

        .alumni-card {
            background: #4a4a4a;
            border-radius: 12px;
            padding: 25px 20px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }

        .alumni-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.4);
        }

        .alumni-card img {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            object-fit: cover;
            margin: 0 auto 15px;
            display: block;
        }

        .alumni-card h3 {
            font-size: 1.1em;
            margin: 10px 0 5px 0;
            color: #ffffff;
        }

        .alumni-role {
            font-size: 0.9em;
            color: #7dd87d;
            font-weight: 600;
            margin: 5px 0;
            text-transform: uppercase;
            letter-spacing: 0.3px;
        }

        .alumni-period {
            font-size: 0.85em;
            color: #999;
            margin: 5px 0 10px 0;
        }

        .alumni-now {
            font-size: 0.9em;
            color: #d0d0d0;
            line-height: 1.5;
            margin-top: 10px;
            padding-top: 10px;
            border-top: 1px solid #5a5a5a;
            font-style: italic;
        }

        /* Publications */
        .publication-filters {
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
            margin-bottom: 2rem;
            padding: 1.5rem;
            background: #4a4a4a;
            border-radius: 10px;
        }

        .filter-group {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .filter-group label {
            font-size: 0.9rem;
            color: #7dd87d;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .filter-group select,
        .filter-group input[type="text"] {
            padding: 0.7rem 1rem;
            border: 2px solid #5a5a5a;
            border-radius: 6px;
            background: #3a3a3a;
            color: #ffffff;
            font-size: 1rem;
            min-width: 200px;
            transition: all 0.3s ease;
        }

        .filter-group select:focus,
        .filter-group input[type="text"]:focus {
            outline: none;
            border-color: #7dd87d;
        }

        .publication-card {
            background: #4a4a4a;
            padding: 2rem;
            margin-bottom: 2rem;
            border-radius: 12px;
            border-left: 5px solid #7dd87d;
            box-shadow: 0 3px 15px rgba(0,0,0,0.3);
            transition: all 0.3s ease;
        }

        .publication-card:hover {
            box-shadow: 0 6px 25px rgba(0,0,0,0.4);
            transform: translateY(-3px);
        }

        .pub-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 1rem;
            margin-bottom: 1rem;
        }

        .pub-header h3 {
            color: #ffffff;
            font-size: 1.3rem;
            line-height: 1.4;
            flex: 1;
        }

        .pub-badges {
            display: flex;
            gap: 0.5rem;
            flex-wrap: wrap;
            flex-shrink: 0;
        }

        .badge {
            padding: 0.4rem 0.8rem;
            border-radius: 20px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .badge-year {
            background: #5a5a5a;
            color: #ffffff;
        }

        .pub-authors {
            color: #d0d0d0;
            margin-bottom: 0.5rem;
            font-size: 1rem;
        }

        .pub-authors strong {
            color: #7dd87d;
        }

        .pub-journal {
            color: #7dd87d;
            font-weight: 600;
            margin-bottom: 1rem;
            font-size: 0.95rem;
        }

        .pub-actions {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            margin-top: 1.5rem;
        }

        .btn-action {
            display: inline-block;
            padding: 0.7rem 1.2rem;
            background: #3a3a3a;
            color: #7dd87d;
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.9rem;
            font-weight: 600;
            border: 2px solid #5a5a5a;
            transition: all 0.3s ease;
        }

        .btn-action:hover {
            background: #7dd87d;
            color: #ffffff;
            border-color: #7dd87d;
            transform: translateY(-2px);
        }

        /* Related Research */
        .related-research-list {
            display: flex;
            flex-direction: column;
            gap: 2rem;
        }

        .related-paper {
            background: #4a4a4a;
            padding: 2rem;
            border-radius: 10px;
            border-left: 4px solid #7dd87d;
            transition: all 0.3s ease;
        }

        .related-paper:hover {
            background: #5a5a5a;
            transform: translateX(5px);
            box-shadow: 0 4px 15px rgba(125, 216, 125, 0.2);
        }

        .related-paper h4 {
            margin: 0 0 1rem 0;
            font-size: 1.3rem;
            line-height: 1.4;
        }

        .related-paper h4 a {
            color: #7dd87d;
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .related-paper h4 a:hover {
            color: #9ff09f;
            text-decoration: underline;
        }

        .paper-authors {
            color: #d0d0d0;
            font-size: 1rem;
            margin: 0.5rem 0;
            font-weight: 300;
        }

        .paper-journal {
            color: #7dd87d;
            font-size: 0.95rem;
            font-style: italic;
            margin: 0.5rem 0 1rem 0;
        }

        .paper-summary {
            color: #b0b0b0;
            font-size: 0.95rem;
            line-height: 1.7;
            margin: 0;
            font-weight: 300;
        }

        /* Contact Section */
        .contact-box {
            background: linear-gradient(135deg, #7dd87d, #6fcfe7);
            padding: 3rem;
            border-radius: 15px;
            color: white;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }

        .contact-box h2 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }

        .contact-box p {
            font-size: 1.2rem;
            margin-bottom: 2rem;
        }

        .contact-email {
            display: inline-block;
            background: white;
            color: #7dd87d;
            padding: 1rem 2.5rem;
            border-radius: 50px;
            font-weight: bold;
            font-size: 1.1rem;
            text-decoration: none;
            transition: all 0.3s ease;
            word-break: break-word;
            max-width: 90%;
        }

        .contact-email:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 25px rgba(0,0,0,0.2);
        }

        /* Footer */
        footer {
            background: var(--dark-bg);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }

        footer p {
            color: #aaa;
        }

        /* Page Visibility */
        .page {
            display: none;
        }

        .page.active {
            display: block;
            animation: fadeIn 0.5s ease-in;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Responsive */
        @media (max-width: 1024px) {
            .alumni-grid {
                grid-template-columns: repeat(3, 1fr);
            }
        }

        @media (max-width: 768px) {
            .mobile-toggle {
                display: block;
                background: none;
                border: 2px solid #7dd87d;
                color: #7dd87d;
                font-size: 1.5rem;
                padding: 0.5rem 1rem;
                border-radius: 4px;
                cursor: pointer;
            }

            nav ul {
                display: none;
                flex-direction: column;
                position: absolute;
                top: 100%;
                left: 0;
                width: 100%;
                background: #2a2a2a;
                padding: 1rem;
                box-shadow: 0 10px 20px rgba(0,0,0,0.3);
                border-top: 2px solid #7dd87d;
                z-index: 1000;
                gap: 0;
            }

            nav ul.active {
                display: flex;
            }

            nav ul li {
                margin: 0;
            }

            nav ul a {
                color: #ffffff;
                display: block;
                padding: 1rem;
                border-bottom: 1px solid #3a3a3a;
            }

            nav ul a:hover,
            nav ul a.active {
                background: #7dd87d;
                color: #ffffff;
            }

            .video-divider {
                height: 25vh;
            }

            #research .video-divider,
            #publications .video-divider,
            #participate .video-divider {
                height: 18vh;
            }

            .logo-center {
                padding: 40px 20px;
            }

            .logo-center img {
                width: 280px;
                max-width: 90%;
            }

            .team-member.pi {
                flex-direction: column;
                text-align: center;
                padding: 30px 20px;
            }

            .team-member.pi img {
                width: 150px;
                height: 150px;
            }

            .team-section {
                padding: 0 20px;
                margin: 40px auto;
            }

            .team-grid {
                grid-template-columns: 1fr;
                gap: 30px;
            }

            .alumni-section {
                padding: 0 20px;
            }

            .alumni-grid {
                grid-template-columns: repeat(2, 1fr);
                gap: 20px;
            }

            .research-grid,
            .image-grid {
                grid-template-columns: 1fr;
            }

            /* Research inline images su mobile */
            .research-text-content {
                font-size: 1rem;
                text-align: left;
            }
            
            .research-text-content h3 {
                font-size: 1.3rem;
                margin: 2rem 0 1rem 0;
            }
            
            .research-inline-image,
            .research-inline-image:nth-of-type(2n) {
                float: none;
                width: 100%;
                max-width: 100%;
                margin: 1.5rem 0;
            }

            /* Publications filters su mobile */
            .publication-filters {
                flex-direction: column;
            }
            
            .filter-group select,
            .filter-group input[type="text"] {
                min-width: 100%;
            }
            
            .pub-header {
                flex-direction: column;
            }
            
            .pub-badges {
                justify-content: flex-start;
            }
            
            .pub-actions {
                flex-direction: column;
            }
            
            .btn-action {
                text-align: center;
            }

            .related-paper {
                padding: 1.5rem;
            }
            
            .related-paper h4 {
                font-size: 1.1rem;
            }

            /* FIX EMAIL PARTICIPATE SU MOBILE */
            .contact-email {
                font-size: 0.9rem;
                padding: 0.8rem 1.5rem;
                max-width: 95%;
                overflow-wrap: break-word;
            }

            .contact-box h2 {
                font-size: 1.8rem;
            }

            .contact-box p {
                font-size: 1rem;
            }
        }

        @media (max-width: 480px) {
            .alumni-grid {
                grid-template-columns: 1fr;
            }

            /* Email ancora piÃ¹ piccola su schermi molto piccoli */
            .contact-email {
                font-size: 0.8rem;
                padding: 0.7rem 1rem;
            }
        }
    </style>
</head>
<body>

<!-- Header -->
<header>
    <div class="header-content">
        <div class="logo-text">The Dynamic Perception Lab</div>
        <button class="mobile-toggle" onclick="toggleMenu()">â˜°</button>
        <nav>
            <ul id="menu">
                <li><a href="#" onclick="showPage('home', this); return false;" class="active">HOME</a></li>
                <li><a href="#" onclick="showPage('people', this); return false;">PEOPLE</a></li>
                <li><a href="#" onclick="showPage('research', this); return false;">RESEARCH</a></li>
                <li><a href="#" onclick="showPage('publications', this); return false;">PUBLICATIONS</a></li>
                <li><a href="#" onclick="showPage('participate', this); return false;">PARTICIPATE</a></li>
            </ul>
        </nav>
    </div>
</header>

<!-- HOME PAGE -->
<div id="home" class="page active">
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="hero-video.mp4" type="video/mp4">
        </video>
    </section>
    
    <div class="logo-center">
        <img src="lab-logo-transparent.png" alt="Dynamic Perception Lab">
    </div>
    
    <div class="container">
        <section class="section">
            <p style="font-size: 1.2rem; line-height: 1.8; color: #e0e0e0;">
                <strong>Our interaction with the physical world feels effortless, but it is anything but simple.</strong> From packing fragile groceries to stacking glassware or navigating a cluttered cabinet, every action we take depends on rapid predictions about stability, weight, balance, and motion. These predictions are generated continuously, online, and with remarkable precisionâ€”often without entering awareness. Seeing the world is only the first step; understanding how it will behave is what allows us to act.
            </p>
        </section>

        <section class="section">
            <h2 class="section-title">Dynamic Perception in Action</h2>
            <div class="image-grid">
                <img src="pool-physics.png" alt="Physical prediction in pool">
                <img src="car-balance.jpg" alt="Stability and balance">
                <img src="layers.jpg" alt="Fluid dynamics"> 
            </div>
        </section>
    </div>
</div>

<!-- PEOPLE PAGE -->
<div id="people" class="page">
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="people-video.mp4" type="video/mp4">
        </video>
    </section>
    
    <div class="container">
        <section class="team-section">
            <h2>Our Team</h2>
            
            <div class="team-member pi">
                <img src="jason-fischer.png" alt="Dr. Jason Fischer">
                <div class="member-info">
                    <h3>Dr. Jason Fischer</h3>
                    <p class="role">Principal Investigator</p>
                    <p class="bio">Leads research at the intersection of visual perception and cognition, with a focus on intuitive physicsâ€”the mental systems that allow us to understand and predict physical behavior. His work explores how the brain transforms visual information into physical knowledge that guides everyday interactions with the world.</p>
                </div>
            </div>

            <div class="team-grid">
                <div class="team-member">
                    <img src="alex-mitko.png" alt="Alex Mitko">
                    <div class="member-info">
                        <h3>Alex Mitko</h3>
                        <p class="role">PhD Student</p>
                        <p class="bio">Fifth-year doctoral researcher investigating the cognitive architecture underlying intuitive physics. His work examines how the mind performs complex physical judgments without explicit mathematical knowledgeâ€”a capability we use effortlessly every day. Beyond the lab, he's an enthusiast of stand-up comedy, football, and hiking.</p>
                    </div>
                </div>

                <div class="team-member">
                    <img src="giuliana-bucci.png" alt="Giuliana Bucci-Mansilla">
                    <div class="member-info">
                        <h3>Giuliana Bucci-Mansilla</h3>
                        <p class="role">PhD Student</p>
                        <p class="bio">Explores visual dynamics in complex environments and their connection to cognitive processes like decision-making. Her research extends to comparative cognition, examining how different primate species solve physics problems to uncover shared and unique mechanisms across species. She champions ecologically valid experiments to understand real-world brain function and behavior. Outside research, she pursues hiking, camping, rock climbing, and wildlife photography.</p>
                    </div>
                </div>
            </div>

            <div class="team-grid">
                <div class="team-member">
                    <img src="garrett-goldin.png" alt="Garrett Goldin">
                    <div class="member-info">
                        <h3>Garrett Goldin</h3>
                        <p class="role">Master's Student</p>
                        <p class="bio">Neuroscience B.S./M.S. student investigating how humans perceive, interpret, and respond to dynamic physical environments. His research focuses on how physical interaction with the world shapes neural processing and predictive capabilities. When not in the lab, he enjoys skiing, ultimate frisbee, and playing guitar, bass, and drums.</p>
                    </div>
                </div>

                <div class="team-member">
                    <img src="samer-aslan.png" alt="Samer Aslan">
                    <div class="member-info">
                        <h3>Samer Aslan</h3>
                        <p class="role">Master's Student</p>
                        <p class="bio">Computer Science B.S./M.S. student applying deep learning models to physical prediction tasks, bridging artificial intelligence and cognitive science approaches to understanding intuitive physics.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="alumni-section">
            <h2>Alumni</h2>
            <p class="alumni-intro">Our lab alumni have gone on to pursue outstanding careers in academia, industry, and beyond.</p>
            
            <div class="alumni-grid">
                <div class="alumni-card">
                    <img src="sarah-cormiea.png" alt="Dr. Sarah Cormiea">
                    <h3>Dr. Sarah Cormiea</h3>
                    <p class="alumni-role">Graduate Student</p>
                    <p class="alumni-period">2017-2022</p>
                    <p class="alumni-now">Postdoctoral Scholar<br>University of Pennsylvania</p>
                </div>

                <div class="alumni-card">
                    <img src="ana-navarro.png" alt="Dr. Ana Navarro-Cebrian">
                    <h3>Dr. Ana Navarro-Cebrian</h3>
                    <p class="alumni-role">Research Scientist</p>
                    <p class="alumni-period">2019-2021</p>
                    <p class="alumni-now">Lecturer<br>University of Maryland, College Park</p>
                </div>

                <div class="alumni-card">
                    <img src="taylor-washington.png" alt="Taylor Washington">
                    <h3>Taylor Washington</h3>
                    <p class="alumni-role">Research Program Coordinator</p>
                    <p class="alumni-period">2019-2021</p>
                </div>

                <div class="alumni-card">
                    <img src="li-guo.png" alt="Dr. Li Guo">
                    <h3>Dr. Li Guo</h3>
                    <p class="alumni-role">Graduate Student</p>
                    <p class="alumni-period">2016-2020</p>
                    <p class="alumni-now">User Experience Researcher<br>Google</p>
                </div>

                <div class="alumni-card">
                    <img src="florence-campana.png" alt="Dr. Florence Campana">
                    <h3>Dr. Florence Campana</h3>
                    <p class="alumni-role">Postdoctoral Scholar</p>
                    <p class="alumni-period">2017-2018</p>
                </div>

                <div class="alumni-card">
                    <img src="sachi-sanghavi.png" alt="Sachi Sanghavi">
                    <h3>Sachi Sanghavi</h3>
                    <p class="alumni-role">Resident Game Development Guru</p>
                    <p class="alumni-period">2016-2017</p>
                    <p class="alumni-now">Technician<br>MIT</p>
                </div>

                <div class="alumni-card">
                    <img src="sukeun-jeong.png" alt="Dr. Su Keun Jeong">
                    <h3>Dr. Su Keun Jeong</h3>
                    <p class="alumni-role">Postdoctoral Scholar</p>
                    <p class="alumni-period">2016</p>
                    <p class="alumni-now">Assistant Professor<br>Chungbuk National University</p>
                </div>

                <div class="alumni-card">
                    <img src="alissa-lutz.png" alt="Alissa Lutz">
                    <h3>Alissa Lutz</h3>
                    <p class="alumni-role">Research Program Coordinator</p>
                    <p class="alumni-period">2016-2018</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-title">Join Our Team</h2>
            <p style="font-size: 1.1rem; line-height: 1.8; color: #e0e0e0;">
                We're seeking brilliant minds passionate about understanding how perception shapes our physical reality. If you're fascinated by the intersection of vision, cognition, and intuitive physics, we want to hear from you. Check our opportunities or reach out directly to explore collaboration possibilities.
            </p>
        </section>
    </div>
</div>

<!-- RESEARCH PAGE -->
<div id="research" class="page">
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="search.mp4" type="video/mp4">
        </video>
    </section>
    
    <div class="container">
        <section class="section">
            <h2 class="section-title">Research</h2>
            
            <div class="research-text-content">
                <p>Engaging with everyday environments requires far more than passive perception: it depends on a continuous and highly structured understanding of the physical properties and dynamics that govern the world around us. In daily life, humans effortlessly evaluate how objects rest on and support one another, anticipate how they may be acted upon, and predict how they will behave when they fall, roll, collide, or deform. These judgments are typically fast, intuitive, and remarkably precise, allowing us to plan and execute complex actions in real time. Despite their apparent ease, such abilities rely on sophisticated internal representations and computations that operate largely outside conscious awareness.</p>

                <div class="research-inline-image">
                    <img src="researchimg1.png" alt="Physical prediction in billiards">
                </div>

                <p>A growing body of research suggests that these competencies are supported by an intuitive physics systemâ€”a set of cognitive mechanisms that enable people to reason about physical events in a manner that is approximate, probabilistic, and task-adaptive rather than strictly veridical. Work in cognitive science and neuroscience has increasingly converged on the idea that humans rely on internal generative models of the physical world, sometimes described as a mental physics engine, to simulate and predict physical outcomes. Rather than explicitly calculating physical equations, this system appears to operate by simulating likely future states of the world, integrating perceptual input with prior knowledge and experience.</p>

                <p>Research in our lab is aimed at characterizing the mental processes and computations underlying this intuitive understanding of physical structure and dynamics. Specifically, we investigate what kinds of internal operations constitute the core of intuitive physics, how these operations are recruited across different physical scenarios, and how they interact with other cognitive systems. Recent and ongoing work provides evidence that physical reasoning is not monolithic, but instead relies on a flexible combination of perceptual analysis, memory-based expectations, and simulation-like processes that are dynamically modulated by task demands and context.</p>

                <div class="research-inline-image">
                    <img src="researchimg2.png" alt="Balance and stability">
                </div>

                <h3>Dedicated Cognitive Resources</h3>
                
                <p>One central question concerns whether there are dedicated cognitive resources for intuitive physics. Behavioral and neurocognitive evidence suggests that reasoning about physical events engages partially specialized mechanisms that are distinct from, yet closely integrated with, systems for perception, attention, and action planning. For example, studies on physical scene understanding indicate that predictive processing of object interactions can occur rapidly and sometimes independently of explicit attentional focus, while still being influenced by top-down goals and expectations. This raises the possibility that intuitive physics relies on domain-specific representations that are nevertheless embedded within broader perceptualâ€“cognitive architectures.</p>

                <h3>Computational Structure</h3>
                
                <p>Another key line of inquiry addresses the computational structure of the intuitive physics system. Are there mental algorithms tailored to specific classes of events, such as collisions or balance, or to specific types of materials, such as rigid bodies, fluids, or deformable objects like cloth? Evidence from both vision research and action-related studies suggests that different physical properties may be processed with varying degrees of abstraction and sensory grounding.</p>

                <div class="research-inline-image">
                    <img src="researchimg.png" alt="Fluid dynamics">
                </div>

                <h3>Embodied and Multimodal Representations</h3>
                
                <p>Our work also explores how physical reasoning is grounded in embodied interactions with the world. Studies have demonstrated that participants adopt control strategies consistent with Newtonian physics when using natural, sensorimotor interactions, but resort to simpler heuristics when interactions are more abstract. This suggests that the format of interactionâ€”whether embodied or symbolicâ€”fundamentally shapes the nature of physical reasoning.</p>

                <h3>Development and Evolution</h3>
                
                <p>Our work also explores how intuitive physics abilities change across the lifespan and with experience. Extensive training in perceiving and predicting specific physical scenariosâ€”such as planning a billiards shot, balancing objects on a tray, or anticipating the movement of toolsâ€”can lead to measurable improvements in physical reasoning. A crucial question is whether such improvements reflect fine-tuning of general-purpose simulation mechanisms or the acquisition of more specialized representations tied to particular contexts.</p>

                <h3>Interactions with Perception and Memory</h3>
                
                <p>Finally, an important focus of our research concerns how intuitive physics interacts with perception, attention, and memory. Physical predictions are deeply intertwined with perceptual representations: sensory input constrains simulation, while prior knowledge and expectations shape how physical scenes are interpreted. Understanding how intuitive physics is embedded within these broader representational systems is essential for explaining how humans achieve such robust and adaptive interactions with the physical world.</p>
            </div>
        </section>

        <section class="section" style="margin-top: 4rem;">
            <h2 class="section-title">Related Research in the Field</h2>
            <p style="font-size: 1.05rem; line-height: 1.9; color: #d0d0d0; margin-bottom: 2rem; font-weight: 300;">
                The study of intuitive physics draws on insights from multiple disciplines. Below are key papers that have shaped our understanding of how the mind represents and reasons about physical interactions:
            </p>
            
            <div class="related-research-list">
                <div class="related-paper">
                    <h4><a href="Retinotopic_adaptation_reveals_distinct_categories_of_causal_perception.pdf" target="_blank">
                        Retinotopic adaptation reveals distinct categories of causal perception
                    </a></h4>
                    <p class="paper-authors">Kominsky, J. F., & Scholl, B. J.</p>
                    <p class="paper-journal">Cognition, 2023</p>
                    <p class="paper-summary">Psychophysical evidence that visual processing distinguishes different types of causal interactions through retinotopically specific adaptation, revealing that launching and triggering are processed by overlapping mechanisms.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Partial mental simulation explains fallacies in physical reasoning.pdf" target="_blank">
                        Partial mental simulation explains fallacies in physical reasoning
                    </a></h4>
                    <p class="paper-authors">Bass, I., Smith, K. A., Bonawitz, E., & Ullman, T. D.</p>
                    <p class="paper-journal">Cognitive Neuropsychology, 2022</p>
                    <p class="paper-summary">Proposes that systematic errors in physical reasoning arise from partial mental simulations that fail to propagate all physical constraints through time, offering a computational account of the physical conjunction fallacy.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="A Role for Visual Areas in Physics Simulations.pdf" target="_blank">
                        A role for visual areas in physics simulations
                    </a></h4>
                    <p class="paper-authors">Ahuja, A., Desrochers, T. M., & Sheinberg, D. L.</p>
                    <p class="paper-journal">HHS Public Access, 2021</p>
                    <p class="paper-summary">Using fMRI, demonstrates that motion-sensitive brain regions are activated during physical prediction tasks, suggesting that mental simulations recreate sensory depictions of how physical scenes unfold.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Naturalistic embodied interactions elicit intuitive physical behaviour in accordance with Newtonian physics.pdf" target="_blank">
                        Naturalistic embodied interactions elicit intuitive physical behaviour in accordance with Newtonian physics
                    </a></h4>
                    <p class="paper-authors">NeupÃ¤rtl, N., Tatai, F., & Rothkopf, C. A.</p>
                    <p class="paper-journal">Cognitive Neuropsychology, 2021</p>
                    <p class="paper-summary">Shows that participants adopt non-linear control prescribed by Newtonian physics when using embodied, sensorimotor interactions, but use linear heuristics with abstract key-press responses.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Visual Adaptation of the Perception of Causality.pdf" target="_blank">
                        Visual adaptation of the perception of causality
                    </a></h4>
                    <p class="paper-authors">Rolfs, M., Dambacher, M., & Cavanagh, P.</p>
                    <p class="paper-journal">Cell Press, 2013</p>
                    <p class="paper-summary">Landmark paper demonstrating that causal perception adapts in retinotopic coordinates after prolonged viewing of collision events, revealing specialized visual routines for detecting cause and effect.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Cochlea to categories The spatiotemporal dynamics of semantic auditory representations.pdf" target="_blank">
                        Cochlea to categories: The spatiotemporal dynamics of semantic auditory representations
                    </a></h4>
                    <p class="paper-authors">Lowe, M. X., Mohsenzadeh, Y., Lahner, B., Charest, I., Oliva, A., & Teng, S.</p>
                    <p class="paper-journal">Cognitive Neuropsychology, 2022</p>
                    <p class="paper-summary">Combining MEG and fMRI to illustrate the progression from acoustic to semantically dominated representations in auditory processing, showing how real-world events are coded via specialized category-specific streams.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Bottom-up and top-down modulation of route selection in imitation.pdf" target="_blank">
                        Bottom-up and top-down modulation of route selection in imitation
                    </a></h4>
                    <p class="paper-authors">Tessari, A., Proietti, R., & Rumiati, R. I.</p>
                    <p class="paper-journal">Cognitive Neuropsychology, 2022</p>
                    <p class="paper-summary">Investigates how the cognitive system selects between semantic and visuomotor processes for action imitation, showing that contextual regularities and cognitive control differentially modulate route selection.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Do capuchin monkeys Sapajus apella use exploration to form intuitions about physical properties .pdf" target="_blank">
                        Do capuchin monkeys use exploration to form intuitions about physical properties?
                    </a></h4>
                    <p class="paper-authors">Jordan, E. J., VÃ¶lter, C. J., & Seed, A. M.</p>
                    <p class="paper-journal">Cognitive Neuropsychology, 2022</p>
                    <p class="paper-summary">Explores the evolutionary origins of intuitive physics by investigating whether capuchin monkeys' spontaneous object exploration supports learning about physical properties.</p>
                </div>

                <div class="related-paper">
                    <h4><a href="Physical understanding in neurodegenerative diseases.pdf" target="_blank">
                        Physical understanding in neurodegenerative diseases
                    </a></h4>
                    <p class="paper-authors">Baumard, J., Lesourd, M., GuÃ©zouli, L., & Osiurak, F.</p>
                    <p class="paper-journal">Cognitive Neuropsychology, 2022</p>
                    <p class="paper-summary">Quantitative review of physical understanding impairments in Alzheimer's disease, semantic dementia, and corticobasal syndrome, showing that intrinsic deficits of physical understanding are observed mainly in late disease stages.</p>
                </div>
            </div>
        </section>
    </div>
</div>


<!-- PUBLICATIONS PAGE - TUTTI I 26 ARTICOLI -->
<div id="publications" class="page">
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="publications.mp4" type="video/mp4">
        </video>
    </section>
        
    <div class="container">
        <section class="section">
            <h2 class="section-title">Publications</h2>
            
            <p style="font-size: 1.05rem; line-height: 1.9; color: #d0d0d0; margin-bottom: 3rem; text-align: justify;">
                This page collects publications related to research on intuitive physics and physical reasoning. The work presented here investigates how humans perceive and predict physical events, form expectations about object interactions, and use physical knowledge to guide action in everyday environments. Using primarily behavioral and cognitive approaches, these studies examine the interaction between perceptual input, prior experience, and predictive mechanisms in shaping physical understanding. Together, these publications contribute to a broader account of how intuitive knowledge of the physical world is structured, learned, and applied across different tasks and contexts.
            </p>
            
            <div class="publication-filters">
                <div class="filter-group">
                    <label>Filter by Year:</label>
                    <select id="yearFilter" onchange="filterPublications()">
                        <option value="all">All Years</option>
                        <option value="2025">2025</option>
                        <option value="2024">2024</option>
                        <option value="2023">2023</option>
                        <option value="2022">2022</option>
                        <option value="2020">2020</option>
                        <option value="2016">2016</option>
                        <option value="2014">2014</option>
                        <option value="2013">2013</option>
                        <option value="2012">2012</option>
                        <option value="2011">2011</option>
                        <option value="2009">2009</option>
                    </select>
                </div>
                
                <div class="filter-group">
                    <input type="text" id="searchBox" placeholder="Search publications..." onkeyup="filterPublications()">
                </div>
            </div>
        </section>

        <section class="section">
            <div id="publicationsList">
                
                <!-- 2025 -->
                <div class="publication-card" data-year="2025">
                    <div class="pub-header">
                        <h3>Visual simulation: Catching a glimpse of what's to come</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2025</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Current Biology, 35(1), R21-R23
                    </div>
                    
                    <div class="pub-actions">
                        <a href="visual simulation catching a glimpse of what s to come.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2024 - Article 1 -->
                <div class="publication-card" data-year="2024">
                    <div class="pub-header">
                        <h3>Physical reasoning is the missing link between action goals and kinematics</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2024</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Physics of Life Reviews, 48, 198-200
                    </div>
                    
                    <div class="pub-actions">
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2024 - Article 2 -->
                <div class="publication-card" data-year="2024">
                    <div class="pub-header">
                        <h3>A dedicated mental resource for intuitive physics</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2024</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Mitko, A.</strong>, Navarroâ€CebriÃ¡n, A., Cormiea, S., & <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        iScience, 27(1)
                    </div>
                    
                    <div class="pub-actions">
                        <a href="A dedicated mental resource for intuitive physics.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2023 - Article 1 -->
                <div class="publication-card" data-year="2023">
                    <div class="pub-header">
                        <h3>Do striking biases in mass inference reflect a flawed mental model of physics?</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2023</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Mitko, A.</strong> & <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Journal of Experimental Psychology: General, 152(9), 2636-2650
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Do Striking Biases in Mass Inference Reflect a Flawed Mental Model of Physics.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2023 - Article 2 -->
                <div class="publication-card" data-year="2023">
                    <div class="pub-header">
                        <h3>Odor discrimination is immune to the effects of verbal labels</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2023</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Cormiea, S. & <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Scientific Reports, 13(1742)
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Odor discrimination is immune to the effects of verbal labels.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2022 - Article 1 -->
                <div class="publication-card" data-year="2022">
                    <div class="pub-header">
                        <h3>Precise functional connections from the dorsal anterior cingulate cortex to the intuitive physics network in the human brain</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Navarro-Cebrian, A. & <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        European Journal of Neuroscience, 56(1)
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Precise functional connections from the dorsal anterior ci...rtex to the intuitive physics network in the human brain.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2022 - Article 2 -->
                <div class="publication-card" data-year="2022">
                    <div class="pub-header">
                        <h3>What tool representation, intuitive physics, and action have in common: The Brain's First-Person Physics Engine</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>, & Mahon, B. Z.
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 38(07â€“08)
                    </div>
                    
                    <div class="pub-actions">
                        <a href="What tool representation_intuitive physics_and action have in common_The brain's first-person physics engine.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2022 - Article 3 -->
                <div class="publication-card" data-year="2022">
                    <div class="pub-header">
                        <h3>The building blocks of intuitive physics in the mind and brain</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Neuropsychology, 38(07â€“08)
                    </div>
                    
                    <div class="pub-actions">
                        <a href="The building blocks of intuitive physics in the mind and brain.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2022 - Article 4 -->
                <div class="publication-card" data-year="2022">
                    <div class="pub-header">
                        <h3>A Survey on Machine Learning Approaches for Modelling Intuitive Physics</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2022</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Duan, J., Dasgupta, A., <strong>Fischer, J.</strong>, & Tan, C.
                    </div>
                    
                    <div class="pub-journal">
                        International Joint Conferences on Artificial Intelligence Organization-ECAI2022
                    </div>
                    
                    <div class="pub-actions">
                        <a href="A Survey on Machine Learning Approaches for Modelling Intuitive Physics.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2020 - Article 1 -->
                <div class="publication-card" data-year="2020">
                    <div class="pub-header">
                        <h3>When it all falls down: the relationship between intuitive physics and spatial cognition</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2020</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Mitko, A.</strong> & <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Cognitive Research: Principles & Implications, 5(24)
                    </div>
                    
                    <div class="pub-actions">
                        <a href="When_it_all_falls_down_the_re.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2020 - Article 2 -->
                <div class="publication-card" data-year="2020">
                    <div class="pub-header">
                        <h3>NaÃ¯ve physics: building a mental model of how the world behaves</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2020</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        In Poeppel, D., Mangun, G. R., & Gazzaniga, M. S. (Eds.) The Cognitive Neurosciences VI, pp. 777-783
                    </div>
                    
                    <div class="pub-actions">
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2020 - Article 3 -->
                <div class="publication-card" data-year="2020">
                    <div class="pub-header">
                        <h3>Knowledge of objects' physical properties implicitly guides attention during visual search</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2020</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Guo, L., Courtney, S. M., & <strong>Fischer, J.</strong>
                    </div>
                    
                    <div class="pub-journal">
                        Journal of Experimental Psychology: General, 149(12), 2332â€“2343
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Knowledge of Objects' Physical Properties Implicitly Guides Attention During Visual Search.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2016 - Article 1 -->
                <div class="publication-card" data-year="2016">
                    <div class="pub-header">
                        <h3>Functional neuroanatomy of intuitive physical inference</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2016</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>, Mikhael, J. G., Tenenbaum, J. B., & Kanwisher, N.
                    </div>
                    
                    <div class="pub-journal">
                        Proc. Natl. Acad. Sci. U.S.A., 113(34):E5072â€“E5081
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Functional neuroanatomy of intuitive physical inference.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2016 - Article 2 -->
                <div class="publication-card" data-year="2016">
                    <div class="pub-header">
                        <h3>Unimpaired attentional disengagement in toddlers with autism spectrum disorder</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2016</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>, Smith, H., Martinezâ€Pedraza, F., Carter, A. S., Kanwisher, N., & Kaldy, Z.
                    </div>
                    
                    <div class="pub-journal">
                        Developmental Science, 19(6): 1095â€“1103
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Unimpaired Attentional Disengagement in Toddlers With Autism Spectrum Disorder.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2014 - Article 1 -->
                <div class="publication-card" data-year="2014">
                    <div class="pub-header">
                        <h3>Serial dependence in the perception of faces</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2014</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Liberman, A., <strong>Fischer, J.</strong>, & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Current Biology, 24(21):2569â€“2574
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Serial Dependence in the Perception of Faces.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2014 - Article 2 -->
                <div class="publication-card" data-year="2014">
                    <div class="pub-header">
                        <h3>Serial dependence in visual perception</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2014</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong> & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Nature Neuroscience, 17:738â€“743
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Serial dependence in visual perception.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2014 - Article 3 -->
                <div class="publication-card" data-year="2014">
                    <div class="pub-header">
                        <h3>The Hierarchical Sparse Selection Model of Visual Crowding</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2014</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Chaney, W., <strong>Fischer, J.</strong>, & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Frontiers in Integrative Neuroscience, 8:73
                    </div>
                    
                    <div class="pub-actions">
                        <a href="The hierarchical sparse selection model of visual crowding.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2014 - Article 4 -->
                <div class="publication-card" data-year="2014">
                    <div class="pub-header">
                        <h3>Ensemble Crowd Perception: A Viewpoint-Invariant Mechanism to Represent Average Crowd Identity</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2014</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Yamanashi Leib, A., <strong>Fischer, J.</strong>, Liu, Y., Qiu, S., Robertson, L., & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Journal of Vision, 14:26
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Ensemble crowd perception A viewpoint invariant mechanism to represent average crowd identity.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2014 - Article 5 -->
                <div class="publication-card" data-year="2014">
                    <div class="pub-header">
                        <h3>Unimpaired Attentional Disengagement and Social Orienting in Children with Autism</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2014</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>*, Koldewyn, K.*, Jiang, Y.V., & Kanwisher, N.
                    </div>
                    
                    <div class="pub-journal">
                        Clinical Psychological Science, 2(2):214â€“223
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Unimpaired Attentional Disengagement and Social Orienting in Children With Autism.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2013 -->
                <div class="publication-card" data-year="2013">
                    <div class="pub-header">
                        <h3>Motion-dependent representation of space in area MT+</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2013</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Maus, G.W., <strong>Fischer, J.</strong>, & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Neuron, 78(3):554â€“562
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Motion-Dependent Representation of Space in Area MTplus.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2012 - Article 1 -->
                <div class="publication-card" data-year="2012">
                    <div class="pub-header">
                        <h3>Attention gates visual coding in the human pulvinar</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2012</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong> & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Nature Communications, 3:1051
                    </div>
                    
                    <div class="pub-actions">
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2012 - Article 2 -->
                <div class="publication-card" data-year="2012">
                    <div class="pub-header">
                        <h3>Crowd Perception in Prosopagnosia</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2012</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Yamanashi Leib, A., Puri, A., <strong>Fischer, J.</strong>, Bentin, S., Whitney, D., & Robertson, L.
                    </div>
                    
                    <div class="pub-journal">
                        Neuropsychologia, 50(7):1698â€“1707
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Crowd perception in prosopagnosia.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2011 - Article 1 -->
                <div class="publication-card" data-year="2011">
                    <div class="pub-header">
                        <h3>Object-level visual information gets through the bottleneck of crowding</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2011</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong> & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Journal of Neurophysiology, 106(3):1389â€“1398
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Object-level visual information gets through the bottleneck of crowding.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2011 - Article 2 -->
                <div class="publication-card" data-year="2011">
                    <div class="pub-header">
                        <h3>The emergence of perceived position in the visual system</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2011</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong>, Spotswood, N, & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Journal of Cognitive Neuroscience, 23(1):119â€“136
                    </div>
                    
                    <div class="pub-actions">
                        <a href="The Emergence of Perceived Position in the Visual System.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2011 - Article 3 -->
                <div class="publication-card" data-year="2011">
                    <div class="pub-header">
                        <h3>Perceived positions determine crowding</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2011</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Maus, G.W., <strong>Fischer, J.</strong>, & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        PLoS ONE, 6(5): e19796
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Perceived Positions Determine Crowding.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2011 - Article 4 -->
                <div class="publication-card" data-year="2011">
                    <div class="pub-header">
                        <h3>Facilitating Stable Representations: Serial Dependence in Vision</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2011</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        Corbett, J., <strong>Fischer, J.</strong>, & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        PLoS ONE, 6(1): e16701
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Facilitating Stable Representations- Serial Dependence in Vision.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2009 - Article 1 -->
                <div class="publication-card" data-year="2009">
                    <div class="pub-header">
                        <h3>Attention narrows position tuning of population responses in V1</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2009</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong> & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Current Biology, 19(16):1356â€“1361
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Attention Narrows Position Tuning of Population Responses in V1.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

                <!-- 2009 - Article 2 -->
                <div class="publication-card" data-year="2009">
                    <div class="pub-header">
                        <h3>Precise discrimination of object position in the human pulvinar</h3>
                        <div class="pub-badges">
                            <span class="badge badge-year">2009</span>
                        </div>
                    </div>
                    
                    <div class="pub-authors">
                        <strong>Fischer, J.</strong> & Whitney, D.
                    </div>
                    
                    <div class="pub-journal">
                        Human Brain Mapping, 30(1):101â€“111
                    </div>
                    
                    <div class="pub-actions">
                        <a href="Precise Discrimination of Object Position in theHuman Pulvinar.pdf" class="btn-action" download>
                            ðŸ“„ Download PDF
                        </a>
                        <a href="https://scholar.google.com/citations?user=aPwpIY0AAAAJ" class="btn-action" target="_blank">
                            ðŸ“š Google Scholar
                        </a>
                    </div>
                </div>

            </div>
            
            <div id="noResults" style="display: none; text-align: center; padding: 3rem; color: #999;">
                <p style="font-size: 1.2rem;">No publications found matching your filters.</p>
            </div>
        </section>
    </div>
</div>

<!-- PARTICIPATE PAGE CON VIDEO E ICONE PNG -->
<div id="participate" class="page">
    <!-- VIDEO BANNER -->
    <section class="video-divider">
        <video autoplay loop muted playsinline>
            <source src="partecipatevideo.mp4" type="video/mp4">
        </video>
    </section>
    
    <div class="container">
        <section class="section">
            <h2 class="section-title">Participate in Our Research</h2>
            <p style="font-size: 1.2rem; line-height: 1.8; margin-bottom: 3rem; color: #e0e0e0;">
                Your participation directly advances our understanding of one of the mind's most fascinating capabilities, how we effortlessly navigate and predict the physical world around us.
            </p>

            <div class="research-grid">
                <div class="research-card">
                    <div class="research-icon">
                        <img src="partecipateimg.png" alt="Why Participate">
                    </div>
                    <h3>Why Participate?</h3>
                    <p>Contribute to cutting-edge cognitive science research and gain exclusive insight into how your brain makes sense of physics.</p>
                </div>
                <div class="research-card">
                    <div class="research-icon">
                        <img src="whattoexpect.png" alt="What to Expect">
                    </div>
                    <h3>What to Expect</h3>
                    <p>View dynamic scenes, make intuitive judgments, engage with innovative experiments. Sessions typically last 30-60 minutes.</p>
                </div>
                <div class="research-card">
                    <div class="research-icon">
                        <img src="partecipateimg2.png" alt="Compensation">
                    </div>
                    <h3>Compensation</h3>
                    <p>Receive course credit or monetary compensation, plus a summary of research findings showing how your data contributes.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="contact-box">
                <h2>Get Involved</h2>
                <p>Ready to contribute to groundbreaking research?</p>
                <a href="mailto:participate@dynamicperceptionlab.com" class="contact-email">participate@dynamicperceptionlab.com</a>
            </div>
        </section>
    </div>
</div>

<!-- Footer -->
<footer>
    <p>&copy; 2026 The Dynamic Perception Lab. All rights reserved.</p>
    <p>University of Coimbra, Portugal</p>
</footer>

<script>
    function showPage(pageId, element) {
        document.querySelectorAll('.page').forEach(page => {
            page.classList.remove('active');
        });
        
        const selectedPage = document.getElementById(pageId);
        if (selectedPage) {
            selectedPage.classList.add('active');
        }
        
        document.querySelectorAll('nav a').forEach(link => {
            link.classList.remove('active');
        });
        if (element) {
            element.classList.add('active');
        }
        
        window.scrollTo({top: 0, behavior: 'smooth'});
        
        const menu = document.getElementById('menu');
        if (menu) {
            menu.classList.remove('active');
        }
    }

    function toggleMenu() {
        const menu = document.getElementById('menu');
        if (menu) {
            menu.classList.toggle('active');
        }
    }

    function filterPublications() {
        const yearFilter = document.getElementById('yearFilter').value;
        const searchBox = document.getElementById('searchBox').value.toLowerCase();
        
        const publications = document.querySelectorAll('.publication-card');
        let visibleCount = 0;
        
        publications.forEach(pub => {
            const year = pub.getAttribute('data-year');
            const text = pub.textContent.toLowerCase();
            
            let showYear = yearFilter === 'all' || year === yearFilter;
            let showSearch = searchBox === '' || text.includes(searchBox);
            
            if (showYear && showSearch) {
                pub.style.display = 'block';
                visibleCount++;
            } else {
                pub.style.display = 'none';
            }
        });
        
        const noResults = document.getElementById('noResults');
        if (visibleCount === 0) {
            noResults.style.display = 'block';
        } else {
            noResults.style.display = 'none';
        }
    }
</script>

</body>
</html>
